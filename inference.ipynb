{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import math\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertJapaneseTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict\n",
    "Candidate = Dict[str, str] \n",
    "\n",
    "\n",
    "import unidic\n",
    "import MeCab\n",
    "home_path = os.environ['HOME']\n",
    "tagger = MeCab.Tagger('-d \"{}\"'.format(unidic.DICDIR))\n",
    "STOP_POSTAGS = ('BOS/EOS',\"代名詞\",\"接続詞\",\"感動詞\",\"動詞,非自立可能\",\"助動詞\",'助詞',\"接頭辞\",\"記号,一般\",\"補助記号\",\"空白\")\n",
    "SEPARATE_TOKEN = '。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_fm_jsonl(f_jsonl: os.path.abspath) -> List[Candidate]:\n",
    "    \"\"\" jsonl -> List[Dict[str, str]] \"\"\"\n",
    "    return [json.loads(line.rstrip()) for line in open(f_jsonl, 'r')]\n",
    "\n",
    "\n",
    "def list_fm_tsv(f_tsv: os.path.abspath, col=0) -> List[int]:\n",
    "    \"\"\" 2cols (pred, out_label_id) -> List[pred:int] \"\"\"\n",
    "    return [int(line.split()[col]) for line in open(f_tsv, 'r')]\n",
    "\n",
    "def search_entity_ignore_answer(queries, topk=1000,ignore_answers=True):\n",
    "    query,answer_candidates = queries\n",
    "    ignore_docid = [entitie2id[answer] for answer in answer_candidates]\n",
    "    \n",
    "    avgdl = sum(doc_id2token_count) / len(doc_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    with open('./ir_dump/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for token in parsed_query:\n",
    "            if token in token2pointer:\n",
    "                pointer, offset = token2pointer[token]\n",
    "                index_file.seek(pointer)\n",
    "                index_line = index_file.read(offset-pointer).rstrip()\n",
    "                postings_list = load_index_line(index_line)\n",
    "                target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(entities)\n",
    "    doc_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for doc_id, tf in postings_list:\n",
    "            dl = doc_id2token_count[doc_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            doc_id2tfidf[doc_id] += token_tfidf\n",
    "    if ignore_answers:\n",
    "        for ignore_id in ignore_docid:\n",
    "            doc_id2tfidf[ignore_id] = 0\n",
    "\n",
    "    docs = [(doc_id, tfidf) for doc_id, tfidf in enumerate(doc_id2tfidf) if tfidf != 0]\n",
    "    docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "    return docs[:topk]\n",
    "\n",
    "def search_entity(queries, topk=1000,ignore_answers=False):\n",
    "    query,answer_candidates = queries\n",
    "    ignore_docid = [entitie2id[answer] for answer in answer_candidates]\n",
    "    \n",
    "    avgdl = sum(doc_id2token_count) / len(doc_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    with open('./ir_dump/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for token in parsed_query:\n",
    "            if token in token2pointer:\n",
    "                pointer, offset = token2pointer[token]\n",
    "                index_file.seek(pointer)\n",
    "                index_line = index_file.read(offset-pointer).rstrip()\n",
    "                postings_list = load_index_line(index_line)\n",
    "                target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(entities)\n",
    "    doc_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for doc_id, tf in postings_list:\n",
    "            dl = doc_id2token_count[doc_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            doc_id2tfidf[doc_id] += token_tfidf\n",
    "    if ignore_answers:\n",
    "        for ignore_id in ignore_docid:\n",
    "            doc_id2tfidf[ignore_id] = 0\n",
    "\n",
    "    docs = [(doc_id, tfidf) for doc_id, tfidf in enumerate(doc_id2tfidf) if tfidf != 0]\n",
    "    docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "    return docs[:topk]\n",
    "\n",
    "def get_contexts_bm25_add_answer(sentence_list,query,answer,topk=1000):\n",
    "    sentence_list = sentence_list.split(\"。\")\n",
    "    inverted_index = defaultdict(list)\n",
    "    sentence_id2sentence = [sentence for sentence in sentence_list]\n",
    "    sentence_id2token_count = []\n",
    "    for sentence_id, sentence in enumerate(sentence_list):\n",
    "        tokens = parse_text(sentence)\n",
    "    \n",
    "        sentence_id2token_count += [len(tokens)]\n",
    "\n",
    "        count_tokens = Counter(tokens)\n",
    "        for token, count in count_tokens.items():\n",
    "            inverted_index[token] += [(sentence_id, count)]\n",
    "\n",
    "    avgdl = sum(sentence_id2token_count) / len(sentence_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    parsed_query += parse_text(answer)\n",
    "    target_posting = {}\n",
    "    for token in parsed_query:\n",
    "        if token in inverted_index:\n",
    "            postings_list = inverted_index[token]\n",
    "            target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(sentence_list)\n",
    "    sentence_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for sentence_id, tf in postings_list:\n",
    "            dl = sentence_id2token_count[sentence_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            sentence_id2tfidf[sentence_id] += token_tfidf\n",
    "\n",
    "    sentences = [(sentence_id, tfidf) for sentence_id, tfidf in enumerate(sentence_id2tfidf) if tfidf != 0]\n",
    "    sentences = sorted(sentences, key=lambda x: x[1], reverse=True)\n",
    "    return \"。\".join(list(map(lambda x: sentence_id2sentence[x[0]], sentences[:topk])))\n",
    "\n",
    "def get_contexts_bm25(sentence_list,query,topk=1000):\n",
    "    sentence_list = sentence_list.split(\"。\")\n",
    "    inverted_index = defaultdict(list)\n",
    "    sentence_id2sentence = [sentence for sentence in sentence_list]\n",
    "    sentence_id2token_count = []\n",
    "    for sentence_id, sentence in enumerate(sentence_list):\n",
    "        tokens = parse_text(sentence)\n",
    "    \n",
    "        sentence_id2token_count += [len(tokens)]\n",
    "\n",
    "        count_tokens = Counter(tokens)\n",
    "        for token, count in count_tokens.items():\n",
    "            inverted_index[token] += [(sentence_id, count)]\n",
    "\n",
    "    avgdl = sum(sentence_id2token_count) / len(sentence_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    for token in parsed_query:\n",
    "        if token in inverted_index:\n",
    "            postings_list = inverted_index[token]\n",
    "            target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(sentence_list)\n",
    "    sentence_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for sentence_id, tf in postings_list:\n",
    "            dl = sentence_id2token_count[sentence_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            sentence_id2tfidf[sentence_id] += token_tfidf\n",
    "\n",
    "    sentences = [(sentence_id, tfidf) for sentence_id, tfidf in enumerate(sentence_id2tfidf) if tfidf != 0]\n",
    "    sentences = sorted(sentences, key=lambda x: x[1], reverse=True)\n",
    "    return \"。\".join(list(map(lambda x: sentence_id2sentence[x[0]], sentences[:topk])))\n",
    "\n",
    "\n",
    "def search_entity_candidates(queries, topk=10):\n",
    "    query,answer_candidates = queries\n",
    "    avgdl = sum(doc_id2token_count) / len(doc_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    with open('./ir_dump/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for token in parsed_query:\n",
    "            if token in token2pointer:\n",
    "                pointer, offset = token2pointer[token]\n",
    "                index_file.seek(pointer)\n",
    "                index_line = index_file.read(offset-pointer).rstrip()\n",
    "                postings_list = load_index_line(index_line)\n",
    "                target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(entities)\n",
    "    doc_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for doc_id, tf in postings_list:\n",
    "            dl = doc_id2token_count[doc_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            doc_id2tfidf[doc_id] += token_tfidf\n",
    "    \n",
    "    # candidateごとの検索\n",
    "    search_results = []\n",
    "    with open('./ir_dump/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for candidate in answer_candidates:\n",
    "            parsed_candidate = parse_text(candidate)\n",
    "            \n",
    "            candidate_target_posting = {}\n",
    "            for token in parsed_candidate:\n",
    "                if token in token2pointer:\n",
    "                    pointer, offset = token2pointer[token]\n",
    "                    index_file.seek(pointer)\n",
    "                    index_line = index_file.read(offset-pointer).rstrip()\n",
    "                    postings_list = load_index_line(index_line)\n",
    "                    candidate_target_posting[token] = postings_list\n",
    "                    \n",
    "            candidate_tfidf = []\n",
    "            # candidateとなる文字列が含まれるdoc_idの集合\n",
    "            candidate_doc_ids = set()\n",
    "            for token_position, (token, postings_list) in enumerate(candidate_target_posting.items()):\n",
    "                idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "                # idfが負になる単語は一般的すぎるので無視\n",
    "                idf = max(idf, 0)\n",
    "                if idf == 0:\n",
    "                    continue\n",
    "                token_doc_ids = []\n",
    "                for doc_id, tf in postings_list:\n",
    "                    dl = doc_id2token_count[doc_id]\n",
    "                    token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "                    doc_id2tfidf[doc_id] += token_tfidf\n",
    "                    candidate_tfidf += [(doc_id, token_tfidf)]\n",
    "                    token_doc_ids += [doc_id]\n",
    "                \n",
    "                if token_position == 0:\n",
    "                    candidate_doc_ids |= set(token_doc_ids)\n",
    "                else:\n",
    "                    candidate_doc_ids &= set(token_doc_ids)\n",
    "\n",
    "            docs = [(doc_id, doc_id2tfidf[doc_id]) for doc_id in candidate_doc_ids]\n",
    "            docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "            search_results += [docs[:topk]]\n",
    "            for doc_id, tfidf in candidate_tfidf:\n",
    "                doc_id2tfidf[doc_id] -= tfidf\n",
    "            \n",
    "    return search_results\n",
    "\n",
    "def parse_text(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.feature.startswith(STOP_POSTAGS):\n",
    "            pass\n",
    "        else:\n",
    "            feature = node.feature.split(\",\")\n",
    "            if len(feature) >7:\n",
    "                tokens += [feature[7].lower()]\n",
    "            else:\n",
    "                tokens += [node.surface.lower()]\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for multiple choice\"\"\"\n",
    "\n",
    "    def __init__(self, example_id, question, contexts, endings,ctx1,ctx2,ctx3,label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            example_id: Unique id for the example.\n",
    "            contexts: list of str. The untokenized text of the first sequence\n",
    "                      (context of corresponding question).\n",
    "            question: string. The untokenized text of the second sequence\n",
    "                      (question).\n",
    "            endings: list of str. multiple choice's options.\n",
    "                     Its length must be equal to contexts' length.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.example_id = example_id\n",
    "        self.question = question\n",
    "        self.contexts = contexts\n",
    "        self.endings = endings\n",
    "        self.label = label\n",
    "        self.ctx1 = ctx1\n",
    "        self.ctx2 = ctx2\n",
    "        self.ctx3 = ctx3\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, example_id, choices_features1,choices_features2,choices_features3,choices_features4, label):\n",
    "        self.example_id = example_id\n",
    "        self.choices_features1 = [\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"segment_ids\": segment_ids,\n",
    "            }\n",
    "            for input_ids, input_mask, segment_ids in choices_features1\n",
    "        ]\n",
    "        self.choices_features2 = [\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"segment_ids\": segment_ids,\n",
    "            }\n",
    "            for input_ids, input_mask, segment_ids in choices_features2\n",
    "        ]\n",
    "        self.choices_features3 = [\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"segment_ids\": segment_ids,\n",
    "            }\n",
    "            for input_ids, input_mask, segment_ids in choices_features3\n",
    "        ]\n",
    "        self.choices_features4 = [\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"segment_ids\": segment_ids,\n",
    "            }\n",
    "            for input_ids, input_mask, segment_ids in choices_features4\n",
    "        ]\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n",
    "\n",
    "    def get_examples(self, mode, data_dir, fname, entities_fname):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "class JaqketProcessor(DataProcessor):\n",
    "\n",
    "    def _get_entities(self, data_dir, entities_fname):\n",
    "        logger.info(\"LOOKING AT {} entities\".format(data_dir))\n",
    "        entities = dict()\n",
    "        for line in self._read_json_gzip(os.path.join(data_dir, entities_fname)):\n",
    "            entity = json.loads(line.strip())\n",
    "            entities[entity[\"title\"]] = entity[\"text\"]\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def get_examples(self, mode, data_dir, json_data, entities, num_options=20):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} [{}]\".format(data_dir, mode))\n",
    "        entities = entities\n",
    "        return self._create_examples(\n",
    "            json_data,\n",
    "            mode,\n",
    "            entities,\n",
    "            num_options,\n",
    "        )\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\n",
    "            \"0\",\n",
    "            \"1\",\n",
    "            \"2\",\n",
    "            \"3\",\n",
    "            \"4\",\n",
    "            \"5\",\n",
    "            \"6\",\n",
    "            \"7\",\n",
    "            \"8\",\n",
    "            \"9\",\n",
    "            \"10\",\n",
    "            \"11\",\n",
    "            \"12\",\n",
    "            \"13\",\n",
    "            \"14\",\n",
    "            \"15\",\n",
    "            \"16\",\n",
    "            \"17\",\n",
    "            \"18\",\n",
    "            \"19\",\n",
    "        ]\n",
    "\n",
    "    def _read_json(self, input_file):\n",
    "        return input_file\n",
    "#         with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "#             lines = fin.readlines()\n",
    "#             return lines\n",
    "\n",
    "    def _read_json_gzip(self, input_file):\n",
    "        with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as fin:\n",
    "            lines = fin.readlines()\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, lines, t_type, entities, num_options):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "\n",
    "        examples = []\n",
    "        skip_examples = 0\n",
    "\n",
    "        # for line in tqdm.tqdm(\n",
    "        #    lines, desc=\"read jaqket data\", ascii=True, ncols=80\n",
    "        # ):\n",
    "        logger.info(\"read jaqket data: {}\".format(len(lines)))\n",
    "        for line in lines:\n",
    "            data_raw = line\n",
    "\n",
    "            id = data_raw[\"qid\"]\n",
    "            question = data_raw[\"question\"].replace(\"_\", \"\")  # \"_\" は cloze question\n",
    "            options = data_raw[\"answer_candidates\"][:num_options]  # TODO\n",
    "            answer = data_raw[\"answer_entity\"]\n",
    "            ctx1 = data_raw[\"ctx1\"]\n",
    "            ctx2 = data_raw[\"ctx2\"]\n",
    "            ctx3 = data_raw[\"ctx3\"]\n",
    "\n",
    "            if len(options) != num_options:\n",
    "                skip_examples += 1\n",
    "                continue\n",
    "\n",
    "            contexts = [entities[options[i]] for i in range(num_options)]\n",
    "            truth = 0\n",
    "\n",
    "            if len(options) == num_options:  # TODO\n",
    "                examples.append(\n",
    "                    InputExample(\n",
    "                        example_id=id,\n",
    "                        question=question,\n",
    "                        contexts=contexts,\n",
    "                        endings=options,\n",
    "                        ctx1=ctx1,\n",
    "                        ctx2=ctx2,\n",
    "                        ctx3=ctx3,\n",
    "                        label=truth,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if t_type == \"train\":\n",
    "            assert len(examples) > 1\n",
    "            assert examples[0].label is not None\n",
    "\n",
    "        logger.info(\"len examples: {}\".format(len(examples)))\n",
    "        logger.info(\"skip examples: {}\".format(skip_examples))\n",
    "\n",
    "        return examples\n",
    "    \n",
    "    \n",
    "def convert_examples_to_features(example):\n",
    "#     tokenizer: PreTrainedTokenizer,)\n",
    "    \n",
    "    \n",
    "    label_list = [f\"{i}\" for i in range(20)]\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    pad_token_segment_id=0\n",
    "    pad_on_left=False\n",
    "    pad_token=0\n",
    "    mask_padding_with_zero=True\n",
    "    max_length = 768\n",
    "    \n",
    "    contexts,endings,question,label,example_id,ctx_add1,ctx_add2,ctx_add3 = example\n",
    "    \n",
    "    ##top1_ignore-answer\n",
    "    entity_text1 = \"。\".join([entities[doc_id2title[s[0]]] for s in ctx_add1[:1]])\n",
    "    ##top5_in-answer\n",
    "    entity_text2 = \"。\".join([entities[doc_id2title[s[0]]] for s in ctx_add2[:5]])\n",
    "    ##top2_ignore-answer\n",
    "    entity_text3 = \"。\".join([entities[doc_id2title[s[0]]] for s in ctx_add1[:2]])\n",
    "    \n",
    "    features = []\n",
    "    context2_1 = get_contexts_bm25(entity_text1,question)\n",
    "    context2_3 = get_contexts_bm25(entity_text2,question)\n",
    "    ##正解エンティティの本文 + 正解候補のタイトルを除外したBM25で引っ張ってきた文章(top1)\n",
    "    choices_features1 = []\n",
    "    ##選択肢本文のみ\n",
    "    choices_features2 = []\n",
    "    ##BM25で引っ張ってきた文章のみ(top5)\n",
    "    choices_features3 = []\n",
    "    ##BM25で引っ張ってきた文章のみ(top5)(wikiを検索するときも並び替えの時もqueryに選択肢を追加)\n",
    "    choices_features4 = []\n",
    "    for ending_idx, (context, ending) in enumerate(\n",
    "        zip(contexts,endings)\n",
    "    ):\n",
    "        input_ids, attention_mask, token_type_ids = make_bert_input1(ending,question,context2_1,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id)\n",
    "        choices_features1.append((input_ids, attention_mask, token_type_ids))\n",
    "        input_ids, attention_mask, token_type_ids = make_bert_input2(ending,question,context2_1,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id)\n",
    "        choices_features2.append((input_ids, attention_mask, token_type_ids))\n",
    "        input_ids, attention_mask, token_type_ids = make_bert_input3(ending,question,context2_3,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id)\n",
    "        choices_features3.append((input_ids, attention_mask, token_type_ids))\n",
    "        \n",
    "        \n",
    "        entity_text = \"。\".join([entities[doc_id2title[s[0]]] for s in ctx_add3[ending_idx][:5]])\n",
    "        context2_4 = get_contexts_bm25_add_answer(entity_text,question,ending)\n",
    "        input_ids, attention_mask, token_type_ids = make_bert_input3(ending,question,context2_4,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id)\n",
    "        choices_features4.append((input_ids, attention_mask, token_type_ids))\n",
    "\n",
    "\n",
    "    label = 0\n",
    "\n",
    "    features.append(\n",
    "        InputFeatures(\n",
    "            example_id=example_id,\n",
    "            choices_features1=choices_features1,\n",
    "            choices_features2=choices_features2,\n",
    "            choices_features3=choices_features3,\n",
    "            choices_features4=choices_features4,\n",
    "            label=label,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "def make_bert_input1(ending,question,context2,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id):\n",
    "    context1 = get_contexts_bm25(entities[ending],question)\n",
    "    text_a = context1[:768]+ tokenizer.sep_token + context2\n",
    "    text_b = question + tokenizer.sep_token + ending\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text_a,\n",
    "        text_b,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_first\",  # 常にcontextをtruncate\n",
    "    )\n",
    "\n",
    "    input_ids, token_type_ids = (\n",
    "        inputs[\"input_ids\"],\n",
    "        inputs[\"token_type_ids\"],\n",
    "    )\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only\n",
    "    # real tokens are attended to.\n",
    "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        attention_mask = (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        ) + attention_mask\n",
    "        token_type_ids = (\n",
    "            [pad_token_segment_id] * padding_length\n",
    "        ) + token_type_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        )\n",
    "        token_type_ids = token_type_ids + (\n",
    "            [pad_token_segment_id] * padding_length\n",
    "        )\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "def make_bert_input2(ending,question,context2,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id):\n",
    "    context1 = get_contexts_bm25(entities[ending],question)\n",
    "    text_a = context1\n",
    "    text_b = question + tokenizer.sep_token + ending\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text_a,\n",
    "        text_b,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_first\",  # 常にcontextをtruncate\n",
    "    )\n",
    "\n",
    "    input_ids, token_type_ids = (\n",
    "        inputs[\"input_ids\"],\n",
    "        inputs[\"token_type_ids\"],\n",
    "    )\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only\n",
    "    # real tokens are attended to.\n",
    "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        attention_mask = (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        ) + attention_mask\n",
    "        token_type_ids = (\n",
    "            [pad_token_segment_id] * padding_length\n",
    "        ) + token_type_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        )\n",
    "        token_type_ids = token_type_ids + (\n",
    "            [pad_token_segment_id] * padding_length\n",
    "        )\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "def make_bert_input3(ending,question,context2,mask_padding_with_zero,max_length,pad_on_left,pad_token,pad_token_segment_id):\n",
    "    context1 = context2\n",
    "    text_a = context1\n",
    "    text_b = question + tokenizer.sep_token + ending\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text_a,\n",
    "        text_b,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_first\",  # 常にcontextをtruncate\n",
    "    )\n",
    "\n",
    "    input_ids, token_type_ids = (\n",
    "        inputs[\"input_ids\"],\n",
    "        inputs[\"token_type_ids\"],\n",
    "    )\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only\n",
    "    # real tokens are attended to.\n",
    "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        attention_mask = (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        ) + attention_mask\n",
    "        token_type_ids = (\n",
    "            [pad_token_segment_id] * padding_length\n",
    "        ) + token_type_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + (\n",
    "            [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        )\n",
    "        token_type_ids = token_type_ids + (\n",
    "            [pad_token_segment_id] * padding_length\n",
    "        )\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "def get_qus_answers(input_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()    \n",
    "    queries = []\n",
    "    answers = []\n",
    "    for line in tqdm(lines):\n",
    "        data_raw = json.loads(line.strip(\"\\n\"))\n",
    "        question = data_raw[\"question\"].replace(\"_\", \"\")  # \"_\" は cloze question\n",
    "        answer = data_raw['answer_candidates']\n",
    "        queries += [(question,answer)]\n",
    "#         answers += [answer]\n",
    "    return queries\n",
    "\n",
    "def load_index_line(index_line):\n",
    "    return list(map(lambda x: tuple(map(int, x.split(':'))), index_line.split(' ')))\n",
    "\n",
    "def read_json(x):\n",
    "    with open(x, \"r\", encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [eval(line) for line in lines]    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_field1(features, field):\n",
    "    return [\n",
    "        [choice[field] for choice in feature.choices_features1] for feature in features\n",
    "    ]\n",
    "\n",
    "def select_field2(features, field):\n",
    "    return [\n",
    "        [choice[field] for choice in feature.choices_features2] for feature in features\n",
    "    ]\n",
    "\n",
    "def select_field3(features, field):\n",
    "    return [\n",
    "        [choice[field] for choice in feature.choices_features3] for feature in features\n",
    "    ]\n",
    "\n",
    "def select_field4(features, field):\n",
    "    return [\n",
    "        [choice[field] for choice in feature.choices_features4] for feature in features\n",
    "    ]\n",
    "\n",
    "def get_inputs(features):\n",
    "    all_input_ids1 = torch.tensor(select_field1(features, \"input_ids\"), dtype=torch.long)\n",
    "    all_input_mask1 = torch.tensor(select_field1(features, \"input_mask\"), dtype=torch.long)\n",
    "    all_segment_ids1 = torch.tensor(select_field1(features, \"segment_ids\"), dtype=torch.long)\n",
    "    all_label_ids1 = torch.tensor([f.label for f in features], dtype=torch.long)  \n",
    "    \n",
    "    all_input_ids2 = torch.tensor(select_field2(features, \"input_ids\"), dtype=torch.long)\n",
    "    all_input_mask2 = torch.tensor(select_field2(features, \"input_mask\"), dtype=torch.long)\n",
    "    all_segment_ids2 = torch.tensor(select_field2(features, \"segment_ids\"), dtype=torch.long)\n",
    "    all_label_ids2 = torch.tensor([f.label for f in features], dtype=torch.long) \n",
    "    \n",
    "    all_input_ids3 = torch.tensor(select_field3(features, \"input_ids\"), dtype=torch.long)\n",
    "    all_input_mask3 = torch.tensor(select_field3(features, \"input_mask\"), dtype=torch.long)\n",
    "    all_segment_ids3 = torch.tensor(select_field3(features, \"segment_ids\"), dtype=torch.long)\n",
    "    all_label_ids3 = torch.tensor([f.label for f in features], dtype=torch.long) \n",
    "    \n",
    "    all_input_ids4 = torch.tensor(select_field4(features, \"input_ids\"), dtype=torch.long)\n",
    "    all_input_mask4 = torch.tensor(select_field4(features, \"input_mask\"), dtype=torch.long)\n",
    "    all_segment_ids4 = torch.tensor(select_field4(features, \"segment_ids\"), dtype=torch.long)\n",
    "    all_label_ids4 = torch.tensor([f.label for f in features], dtype=torch.long) \n",
    "    \n",
    "    return (all_input_ids1,all_input_mask1,all_segment_ids1,all_label_ids1),(all_input_ids2,all_input_mask2,all_segment_ids2,all_label_ids2),(all_input_ids3,all_input_mask3,all_segment_ids3,all_label_ids3),(all_input_ids4,all_input_mask4,all_segment_ids4,all_label_ids4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 106381.52it/s]\n",
      "100%|██████████| 1000/1000 [01:14<00:00, 13.38it/s]\n",
      "100%|██████████| 1000/1000 [01:25<00:00, 11.73it/s]\n",
      "100%|██████████| 1000/1000 [02:45<00:00,  6.03it/s]\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [06:38<00:00,  2.51it/s]\n",
      " 64%|██████▎   | 159/250 [01:31<00:52,  1.73it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3427, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-a9cea1c0e0b0>\", line 117, in <module>\n",
      "    pred = outputs[\"logits\"].cpu().numpy()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2054, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/root/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a9cea1c0e0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mpred1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2053\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2054\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2055\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2054\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2057\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "device = \"cuda:0\"\n",
    "root_path = \"./data/\"\n",
    "test_path = \"./data/aio_leaderboard.json\"\n",
    "\n",
    "with open('./ir_dump/doc_id2title.pickle', 'rb') as f:\n",
    "    doc_id2title = pickle.load(f)\n",
    "with open('./ir_dump/doc_id2token_count.pickle', 'rb') as f:\n",
    "    doc_id2token_count = pickle.load(f)\n",
    "with open('./ir_dump/token2pointer.pickle', 'rb') as f:\n",
    "    token2pointer = pickle.load(f)\n",
    "\n",
    "input_file = './data/all_entities.json.gz'\n",
    "entitie2id = {k:v for v,k in enumerate(doc_id2title)}\n",
    "with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "    \n",
    "entities = dict()\n",
    "for line in lines:\n",
    "    entity = json.loads(line.strip())\n",
    "    entities[entity[\"title\"]] = entity[\"text\"]\n",
    "del lines\n",
    "\n",
    "dev1_queries = get_qus_answers(test_path)\n",
    "with Pool(cpu_count()) as p:\n",
    "    dev1_results = list(tqdm(p.imap(search_entity, dev1_queries), total=len(dev1_queries)))\n",
    "    dev1_results_ignore = list(tqdm(p.imap(search_entity_ignore_answer, dev1_queries), total=len(dev1_queries)))\n",
    "    dev1_results_answer_q = list(tqdm(p.imap(search_entity_candidates, dev1_queries), total=len(dev1_queries)))\n",
    "    \n",
    "processor = JaqketProcessor()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "task_name = \"jaqket\"\n",
    "MODEL_CLASSES = {\"bert\": (BertConfig, BertForMultipleChoice, BertJapaneseTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "path_name = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "config = config_class.from_pretrained(path_name,num_labels=num_labels,finetuning_task=task_name,)\n",
    "tokenizer = tokenizer_class.from_pretrained(path_name)\n",
    "model = model_class.from_pretrained(path_name,config=config)\n",
    "\n",
    "param = model.bert.embeddings.position_embeddings.weight.data\n",
    "param2 = F.interpolate(param.view(1,1,512,768),size=(768,768),mode='bicubic',align_corners=True)[0,0]\n",
    "model.bert.embeddings.position_embeddings.weight = nn.Parameter(param2)\n",
    "\n",
    "model1 = copy.deepcopy(model)\n",
    "model1.load_state_dict(torch.load(\"./params/mix-model-alldata.pt\",map_location=\"cpu\"))\n",
    "model1 = model1.to(device)\n",
    "model1 = amp.initialize(model1, opt_level=\"O2\",verbosity=0)\n",
    "\n",
    "model2 = copy.deepcopy(model)\n",
    "model2.load_state_dict(torch.load(\"./params/title_only-model-alldata.pt\",map_location=\"cpu\"))\n",
    "model2 = model2.to(device)\n",
    "model2 = amp.initialize(model2, opt_level=\"O2\",verbosity=0)\n",
    "\n",
    "model3 = copy.deepcopy(model)\n",
    "model3.load_state_dict(torch.load(\"./params/question_only-model-alldata.pt\",map_location=\"cpu\"))\n",
    "model3 = model3.to(device)\n",
    "model3 = amp.initialize(model3, opt_level=\"O2\",verbosity=0)\n",
    "\n",
    "model4 = copy.deepcopy(model)\n",
    "model4.load_state_dict(torch.load(\"./params/title_only-model-alldata.pt\",map_location=\"cpu\"))\n",
    "model4 = model4.to(device)\n",
    "model4 = amp.initialize(model4, opt_level=\"O2\",verbosity=0)\n",
    "\n",
    "test_data = read_json(test_path)\n",
    "for data,ctx1,ctx2,ctx3 in zip(test_data,dev1_results,dev1_results_ignore,dev1_results_answer_q):\n",
    "    data[\"ctx1\"] = ctx2\n",
    "    data[\"ctx2\"] = ctx1\n",
    "    data[\"ctx3\"] = ctx3\n",
    "\n",
    "\n",
    "test_ex  = processor.get_examples(\"dev\",root_path,test_data,entities)\n",
    "test_values = [(ex.contexts,ex.endings,ex.question,ex.label,ex.example_id,ex.ctx1,ex.ctx2,ex.ctx3) for ex in test_ex]\n",
    "with Pool(multiprocessing.cpu_count()) as p:\n",
    "    test_features = list(tqdm(p.imap(convert_examples_to_features,test_values), total=len(test_values)))\n",
    "    test_features = [f[0] for f in test_features]\n",
    "    \n",
    "batch1,batch2,batch3,batch4 = get_inputs(test_features)\n",
    "\n",
    "batch_size = 4\n",
    "input_batch1 = batch1[0].split(batch_size)\n",
    "att_batch1 = batch1[1].split(batch_size)\n",
    "typeid_batch1 = batch1[2].split(batch_size)\n",
    "label_batch1 = batch1[3].split(batch_size)\n",
    "\n",
    "input_batch2 = batch2[0].split(batch_size)\n",
    "att_batch2 = batch2[1].split(batch_size)\n",
    "typeid_batch2 = batch2[2].split(batch_size)\n",
    "label_batch2 = batch2[3].split(batch_size)\n",
    "\n",
    "input_batch3 = batch3[0].split(batch_size)\n",
    "att_batch3 = batch3[1].split(batch_size)\n",
    "typeid_batch3 = batch3[2].split(batch_size)\n",
    "label_batch3 = batch3[3].split(batch_size)\n",
    "\n",
    "input_batch4 = batch4[0].split(batch_size)\n",
    "att_batch4 = batch4[1].split(batch_size)\n",
    "typeid_batch4 = batch4[2].split(batch_size)\n",
    "label_batch4 = batch4[3].split(batch_size)\n",
    "\n",
    "position_ids = torch.LongTensor([i for i in range(768)]).to(device)\n",
    "with torch.no_grad():\n",
    "    pred1 = []\n",
    "    for batch in tqdm(zip(input_batch1,att_batch1,typeid_batch1,label_batch1),total=len(input_batch1)):\n",
    "        model1.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"position_ids\":position_ids,\n",
    "                    \"labels\": batch[3],\n",
    "                }\n",
    "        outputs = model1(**inputs)\n",
    "        pred = outputs[\"logits\"].cpu().numpy()\n",
    "        pred1.extend(pred)\n",
    "        \n",
    "with torch.no_grad():\n",
    "    pred2 = []\n",
    "    for batch in tqdm(zip(input_batch2,att_batch2,typeid_batch2,label_batch2),total=len(input_batch2)):\n",
    "        model2.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"position_ids\":position_ids,\n",
    "                    \"labels\": batch[3],\n",
    "                }\n",
    "        outputs = model2(**inputs)\n",
    "        pred = outputs[\"logits\"].cpu().numpy()\n",
    "        pred2.extend(pred)\n",
    "        \n",
    "with torch.no_grad():\n",
    "    pred3 = []\n",
    "    for batch in tqdm(zip(input_batch3,att_batch3,typeid_batch3,label_batch3),total=len(input_batch3)):\n",
    "        model3.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"position_ids\":position_ids,\n",
    "                    \"labels\": batch[3],\n",
    "                }\n",
    "        outputs = model3(**inputs)\n",
    "        pred = outputs[\"logits\"].cpu().numpy()\n",
    "        pred3.extend(pred)\n",
    "with torch.no_grad():\n",
    "    pred4 = []\n",
    "    for batch in tqdm(zip(input_batch4,att_batch4,typeid_batch4,label_batch4),total=len(input_batch4)):\n",
    "        model4.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2],\n",
    "                    \"position_ids\":position_ids,\n",
    "                    \"labels\": batch[3],\n",
    "                }\n",
    "        outputs = model4(**inputs)\n",
    "        pred = outputs[\"logits\"].cpu().numpy()\n",
    "        pred4.extend(pred)\n",
    "        \n",
    "answers = list_fm_jsonl(test_path)    # List[Candidate]\n",
    "\n",
    "pred1 = np.array(pred1)\n",
    "pred2 = np.array(pred2)\n",
    "pred3 = np.array(pred3)\n",
    "pred4 = np.array(pred4)\n",
    "pred_labels = ((pred1+pred2+pred3+pred4)/4).argmax(axis=-1)\n",
    "\n",
    "\n",
    "fo = open(\"./submission.json\", 'w')\n",
    "\n",
    "for answer_info, pred_label in zip(answers, pred_labels):\n",
    "    result = {\n",
    "              'qid': answer_info['qid'],\n",
    "              'answer_entity': answer_info['answer_candidates'][pred_label]\n",
    "             }\n",
    "\n",
    "    json.dump(result, fo, ensure_ascii=False)\n",
    "    fo.write('\\n')\n",
    "\n",
    "fo.close()\n",
    "\n",
    "t2 = time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1 = np.array(pred1)\n",
    "# pred2 = np.array(pred2)\n",
    "# pred3 = np.array(pred3)\n",
    "# pred4 = np.array(pred4)\n",
    "\n",
    "## BEST LB\n",
    "w1,w2,w3,w4  = np.array([0.30265671, 0.28305784, 0.17957585, 0.2347096 ])\n",
    "pred1_1,pred1_2,pred1_3,pred1_4 = pred1.copy(),pred2.copy(),pred3.copy(),pred4.copy()\n",
    "\n",
    "pred1_1 *= w1\n",
    "pred1_2 *= w2\n",
    "pred1_3 *= w3\n",
    "pred1_4 *= w4\n",
    "\n",
    "pred_labels = (pred1_1+pred1_2+pred1_3+pred1_4).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = list_fm_jsonl(test_path)    # List[Candidate]\n",
    "\n",
    "# pred1 = np.array(pred1)\n",
    "# pred2 = np.array(pred2)\n",
    "# pred3 = np.array(pred3)\n",
    "# pred4 = np.array(pred4)\n",
    "# pred_labels = ((pred1+pred2+pred3+pred4)/4).argmax(axis=-1)\n",
    "\n",
    "\n",
    "fo = open(\"./submission-searchv3-4model-weight-ensemble-softmax-t03.json\", 'w')\n",
    "\n",
    "for answer_info, pred_label in zip(answers, pred_labels):\n",
    "    result = {\n",
    "              'qid': answer_info['qid'],\n",
    "              'answer_entity': answer_info['answer_candidates'][pred_label]\n",
    "             }\n",
    "\n",
    "    json.dump(result, fo, ensure_ascii=False)\n",
    "    fo.write('\\n')\n",
    "\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py386",
   "language": "python",
   "name": "py386"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
